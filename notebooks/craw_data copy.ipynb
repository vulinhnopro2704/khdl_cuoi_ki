{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f51f291",
   "metadata": {},
   "source": [
    "# 2. Craw product by Link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e7c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.core import driver_cache\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.jomashop.com/watches.html?p={}\"\n",
    "BATCH_SIZE = 5\n",
    "MAX_WORKERS = 8\n",
    "OUTPUT_FILE = \"../data/products.csv\"\n",
    "LINKS_FILE = \"../data/links.csv\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "DRIVER_SETUP_RETRIES = 3\n",
    "DRIVER_SETUP_DELAY = 5\n",
    "IMPLICIT_WAIT = 5.0  # seconds\n",
    "SCROLL_PAUSE = 2.0  # seconds between scrolls\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -----------------------------------\n",
    "# Driver setup with retry\n",
    "# -----------------------------------\n",
    "def setup_driver_service():\n",
    "    \"\"\"Download or fetch ChromeDriver binary with retry logic.\"\"\"\n",
    "    for attempt in range(1, DRIVER_SETUP_RETRIES + 1):\n",
    "        try:\n",
    "            driver_path = ChromeDriverManager().install()\n",
    "            return Service(driver_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Driver setup attempt {attempt} failed: {e}\")\n",
    "            time.sleep(DRIVER_SETUP_DELAY)\n",
    "    logger.error(f\"All {DRIVER_SETUP_RETRIES} chrome driver setups failed\")\n",
    "    raise RuntimeError(\"Unable to install ChromeDriver\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Driver initialization\n",
    "# -----------------------------------\n",
    "def init_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = setup_driver_service()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.implicitly_wait(IMPLICIT_WAIT)\n",
    "    return driver\n",
    "\n",
    "# -----------------------------------\n",
    "# Page scrolling utility\n",
    "# -----------------------------------\n",
    "\n",
    "def scroll_down_slowly(driver: webdriver.Chrome, pause_time: float = 2.0, max_scrolls: int = 30) -> None:\n",
    "    print(\"Scrolling down the page...\")\n",
    "    scroll_count = 0\n",
    "\n",
    "    while scroll_count < max_scrolls:\n",
    "        # Cuộn xuống 2000 pixel\n",
    "        driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "        # Lấy chiều cao trang và vị trí hiện tại\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_position = driver.execute_script(\"return window.scrollY + window.innerHeight\")\n",
    "\n",
    "        print(f\"Scroll #{scroll_count + 1} | Scroll position: {scroll_position:.0f} | Total height: {scroll_height:.0f}\")\n",
    "\n",
    "        # Nếu đã cuộn tới đáy, dừng lại\n",
    "        if scroll_position >= scroll_height:\n",
    "            print(\"Reached bottom of the page.\")\n",
    "            break\n",
    "\n",
    "        scroll_count += 1\n",
    "\n",
    "    if scroll_count >= max_scrolls:\n",
    "        print(\"Reached max scroll limit.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Link extraction per page with retry\n",
    "# -----------------------------------\n",
    "\n",
    "def find_product_link(prod_element, max_strategy_retries=2):\n",
    "    \"\"\"\n",
    "    Thử lần lượt nhiều cách khác nhau để extract href từ prod_element:\n",
    "      1. Dùng data-scroll-target attribute\n",
    "      2. CSS selector (.productItemBlock a)\n",
    "      3. className productImg-link\n",
    "      4. className productName-link\n",
    "      5. XPath\n",
    "      6. JS querySelector\n",
    "      7. find_elements + filter tất cả <a>\n",
    "    \"\"\"\n",
    "    strategies = [\n",
    "        # 1. data-scroll-target trên .productItemBlock\n",
    "        lambda el: el.find_element(By.CLASS_NAME, \"productItemBlock\").get_attribute(\"data-scroll-target\"),\n",
    "        # 2. CSS selector\n",
    "        lambda el: el.find_element(By.CSS_SELECTOR, \".productItemBlock a\").get_attribute(\"href\"),\n",
    "        # 3. className productImg-link\n",
    "        lambda el: el.find_element(By.CLASS_NAME, \"productImg-link\").get_attribute(\"href\"),\n",
    "        # 4. className productName-link\n",
    "        lambda el: el.find_element(By.CLASS_NAME, \"productName-link\").get_attribute(\"href\"),\n",
    "        # 5. XPath\n",
    "        lambda el: el.find_element(By.XPATH, \".//div[contains(@class,'productItemBlock')]//a\").get_attribute(\"href\"),\n",
    "        # 6. JS querySelector\n",
    "        lambda el: el.parent.execute_script(\n",
    "            \"return arguments[0].querySelector('.productItemBlock a').href;\", el),\n",
    "        # 7. fallback: scan tất cả <a>\n",
    "        lambda el: next(\n",
    "            (a.get_attribute(\"href\") for a in el.find_elements(By.TAG_NAME, \"a\")\n",
    "             if a.get_attribute(\"href\")), None\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    last_exception = None\n",
    "    for strat_idx, strat in enumerate(strategies, start=1):\n",
    "        for attempt in range(1, max_strategy_retries + 1):\n",
    "            try:\n",
    "                href = strat(prod_element)\n",
    "                if href:\n",
    "                    # Nếu URL là relative (ví dụ bắt được từ data-scroll-target), nối thêm domain\n",
    "                    if href.startswith(\"/\"):\n",
    "                        href = urljoin(BASE_URL, href)\n",
    "                    print(f\"Strategy #{strat_idx} succeeded on attempt {attempt}: {href}\")\n",
    "                    return href\n",
    "                else:\n",
    "                    raise NoSuchElementException(\"Empty href\")\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                print(f\"  Strategy #{strat_idx} attempt {attempt} failed: {e}\")\n",
    "                time.sleep(0.3)\n",
    "        print(f\"→ Strategy #{strat_idx} exhausted, chuyển sang chiến lược tiếp theo.\")\n",
    "    print(f\"Tất cả chiến lược đều thất bại cho phần tử: {last_exception}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_links(page: int, max_retries: int = MAX_RETRIES) -> list:\n",
    "    logger.info(f\"Extracting links from page {page}\")\n",
    "    url = BASE_URL.format(page)\n",
    "    print(f\"Fetching page {page}: {url}\")\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            driver = init_driver()\n",
    "            driver.get(url)\n",
    "\n",
    "            print(\"Scrolling to load products...\")\n",
    "            scroll_down_slowly(driver)\n",
    "\n",
    "            WebDriverWait(driver, IMPLICIT_WAIT).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, \"productItem\"))\n",
    "            )\n",
    "            products = driver.find_elements(By.CLASS_NAME, \"productItem\")\n",
    "            print(f\"  Found {len(products)} product items on page {page}\")\n",
    "\n",
    "            links = []\n",
    "            for idx, prod in enumerate(products, start=1):\n",
    "                href = find_product_link(prod)\n",
    "                if href:\n",
    "                    links.append(href)\n",
    "                else:\n",
    "                    logger.error(f\"Page {page}, product #{idx}: không lấy được link.\")\n",
    "\n",
    "            logger.info(f\"Page {page}: extracted {len(links)} links on attempt {attempt}\")\n",
    "            return links\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt} failed for page {page}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    logger.error(f\"All {max_retries} attempts failed for page {page}\")\n",
    "    return []\n",
    "\n",
    "# -----------------------------------\n",
    "# Detailed product scraping with retry\n",
    "# -----------------------------------\n",
    "def scrape_product_with_retry(url: str, max_retries: int = MAX_RETRIES) -> dict:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            driver = init_driver()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{url}: driver init failed: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            try:\n",
    "                more_btn = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"show-text\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", more_btn)\n",
    "            except Exception:\n",
    "                pass\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"more-detail-content\"))\n",
    "            )\n",
    "            price = driver.find_element(By.CLASS_NAME, \"now-price\").text.strip()\n",
    "            specs = driver.find_elements(By.CLASS_NAME, \"more-detail-content\")\n",
    "            record = {\"URL\": url, \"Price\": price}\n",
    "            for spec in specs:\n",
    "                try:\n",
    "                    label = spec.find_element(By.CLASS_NAME, \"more-label\").text.strip().replace(' ', '_')\n",
    "                    value = spec.find_element(By.CLASS_NAME, \"more-value\").text.strip()\n",
    "                    record[label] = value\n",
    "                except Exception:\n",
    "                    continue\n",
    "            logger.info(f\"Scraped details for {url} on attempt {attempt}\")\n",
    "            return record\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    logger.error(f\"All {max_retries} attempts failed for {url}\")\n",
    "    return {\"URL\": url, \"Error\": \"Failed after retries\"}\n",
    "\n",
    "def scrape_product_bs(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Scrape product details (price and specs) using requests + BeautifulSoup.\n",
    "    \"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    record = {\"URL\": url}\n",
    "\n",
    "    # 1. Giá ngay lập tức có trong HTML\n",
    "    price_tag = soup.select_one(\".now-price\")\n",
    "    record[\"Price\"] = price_tag.get_text(strip=True) if price_tag else None\n",
    "\n",
    "    # 2. Thông số chi tiết (các block .more-detail-content)\n",
    "    for block in soup.select(\".more-detail-content\"):\n",
    "        label_tag = block.select_one(\".more-label\")\n",
    "        value_tag = block.select_one(\".more-value\")\n",
    "        if label_tag and value_tag:\n",
    "            key = label_tag.get_text(strip=True).replace(\" \", \"_\")\n",
    "            record[key] = value_tag.get_text(strip=True)\n",
    "\n",
    "    return record\n",
    "\n",
    "\n",
    "def scrape_product_bs_with_retry(url: str, max_retries: int = MAX_RETRIES) -> dict:\n",
    "    \"\"\"\n",
    "    Bọc retry quanh hàm scrape_product_bs, tương tự cấu trúc Selenium version.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            record = scrape_product_bs(url)\n",
    "            logger.info(f\"Scraped with BS for {url} on attempt {attempt}\")\n",
    "            return record\n",
    "        except requests.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "    logger.error(f\"All {max_retries} attempts failed for {url}\")\n",
    "    return {\"URL\": url, \"Error\": \"Failed after retries\"}\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# CSV persistence\n",
    "# -----------------------------------\n",
    "def save_to_csv(record: dict, filename: str = OUTPUT_FILE) -> None:\n",
    "    df_new = pd.DataFrame([record])\n",
    "    if os.path.exists(filename):\n",
    "        df_old = pd.read_csv(filename)\n",
    "        df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df = df_new\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# -----------------------------------\n",
    "# Main processing: link extraction & detail scraping with logs and progress bars\n",
    "# -----------------------------------\n",
    "def main():\n",
    "    # # Step 1: Extract links with retry and progress bars\n",
    "    # pages = random.sample(range(1, 500), 912)\n",
    "    # all_links = []\n",
    "    # batches = [pages[i:i + BATCH_SIZE] for i in range(0, len(pages), BATCH_SIZE)]\n",
    "    # for batch_num, batch in enumerate(batches, start=1):\n",
    "    #     logger.info(f\"Starting link batch {batch_num}/{len(batches)}: {batch}\")\n",
    "    #     with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    #         futures = {executor.submit(extract_product_links, p): p for p in batch}\n",
    "    #         for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Links batch {batch_num}\"):\n",
    "    #             page = futures[future]\n",
    "    #             links = future.result()\n",
    "    #             if links and len(links) > 0:\n",
    "    #                 logger.info(f\"Page {page}: found {len(links)} links\")\n",
    "    #                 all_links.extend(links)\n",
    "    #             else:\n",
    "    #                 logger.warning(f\"No links on page {page} after retries\")\n",
    "    #     logger.info(f\"Completed link batch {batch_num}/{len(batches)}\")\n",
    "\n",
    "    # pd.DataFrame({\"Product_URL\": all_links}).to_csv(LINKS_FILE, index=False)\n",
    "    # logger.info(f\"Total links collected: {len(all_links)}\")\n",
    "    # Get all links from CSV\n",
    "    df_links = pd.read_csv(LINKS_FILE)\n",
    "    all_links = df_links[\"Product_URL\"].tolist()[2517:]\n",
    "    # Step 2: Scrape product details in parallel with retry\n",
    "    logger.info(f\"Starting detail scraping for {len(all_links)} products with {MAX_WORKERS} threads\")\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(scrape_product_bs_with_retry, url) for url in all_links]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping products\"):\n",
    "            record = future.result()\n",
    "            save_to_csv(record)\n",
    "    logger.info(\"Scraping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01ec4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in e:\\dut_linh\\hk6\\khdl\\final_term_2\\.venv\\lib\\site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e6711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 10:07:47,098 - INFO - Starting detail scraping for 28407 products with 8 threads\n",
      "Scraping products:   0%|          | 0/28407 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
