{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f51f291",
   "metadata": {},
   "source": [
    "# 2. Craw product by Link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.core import driver_cache\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://www.jomashop.com/watches.html?p={}\"\n",
    "BATCH_SIZE = 10\n",
    "MAX_WORKERS = 3\n",
    "OUTPUT_FILE = \"../data/products.csv\"\n",
    "LINKS_FILE = \"../data/links.csv\"\n",
    "MAX_RETRIES = 2\n",
    "RETRY_DELAY = 1\n",
    "DRIVER_SETUP_RETRIES = 2\n",
    "DRIVER_SETUP_DELAY = 3\n",
    "IMPLICIT_WAIT = 3.0  # seconds\n",
    "SCROLL_PAUSE = 1.0  # seconds between scrolls\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -----------------------------------\n",
    "# Driver setup with retry\n",
    "# -----------------------------------\n",
    "def setup_driver_service():\n",
    "    \"\"\"Download or fetch ChromeDriver binary with retry logic.\"\"\"\n",
    "    for attempt in range(1, DRIVER_SETUP_RETRIES + 1):\n",
    "        try:\n",
    "            driver_path = ChromeDriverManager().install()\n",
    "            return Service(driver_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Driver setup attempt {attempt} failed: {e}\")\n",
    "            time.sleep(DRIVER_SETUP_DELAY)\n",
    "    logger.error(f\"All {DRIVER_SETUP_RETRIES} chrome driver setups failed\")\n",
    "    raise RuntimeError(\"Unable to install ChromeDriver\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Driver initialization\n",
    "# -----------------------------------\n",
    "def init_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = setup_driver_service()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.implicitly_wait(IMPLICIT_WAIT)\n",
    "    return driver\n",
    "\n",
    "# -----------------------------------\n",
    "# Page scrolling utility\n",
    "# -----------------------------------\n",
    "\n",
    "def scroll_down_slowly(driver: webdriver.Chrome, pause_time: float = 2.0, max_scrolls: int = 30) -> None:\n",
    "    print(\"Scrolling down the page...\")\n",
    "    scroll_count = 0\n",
    "\n",
    "    while scroll_count < max_scrolls:\n",
    "        # Cuộn xuống 2000 pixel\n",
    "        driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "        # Lấy chiều cao trang và vị trí hiện tại\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_position = driver.execute_script(\"return window.scrollY + window.innerHeight\")\n",
    "\n",
    "        print(f\"Scroll #{scroll_count + 1} | Scroll position: {scroll_position:.0f} | Total height: {scroll_height:.0f}\")\n",
    "\n",
    "        # Nếu đã cuộn tới đáy, dừng lại\n",
    "        if scroll_position >= scroll_height:\n",
    "            print(\"Reached bottom of the page.\")\n",
    "            break\n",
    "\n",
    "        scroll_count += 1\n",
    "\n",
    "    if scroll_count >= max_scrolls:\n",
    "        print(\"Reached max scroll limit.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Link extraction per page with retry\n",
    "# -----------------------------------\n",
    "\n",
    "def find_product_link(prod_element, max_strategy_retries=2):\n",
    "    \"\"\"\n",
    "    Thử lần lượt nhiều cách khác nhau để extract href từ prod_element:\n",
    "      1. Dùng data-scroll-target attribute\n",
    "      2. CSS selector (.productItemBlock a)\n",
    "      3. className productImg-link\n",
    "      4. className productName-link\n",
    "      5. XPath\n",
    "      6. JS querySelector\n",
    "      7. find_elements + filter tất cả <a>\n",
    "    \"\"\"\n",
    "    strategies = [\n",
    "        # 1. data-scroll-target trên .productItemBlock\n",
    "        lambda el: el.find_element(By.CLASS_NAME, \"productItemBlock\").get_attribute(\"data-scroll-target\"),\n",
    "        # 2. CSS selector\n",
    "        lambda el: el.find_element(By.CSS_SELECTOR, \".productItemBlock a\").get_attribute(\"href\"),\n",
    "        # 3. className productImg-link\n",
    "        lambda el: el.find_element(By.CLASS_NAME, \"productImg-link\").get_attribute(\"href\"),\n",
    "        # 4. className productName-link\n",
    "        lambda el: el.find_element(By.CLASS_NAME, \"productName-link\").get_attribute(\"href\"),\n",
    "        # 5. XPath\n",
    "        lambda el: el.find_element(By.XPATH, \".//div[contains(@class,'productItemBlock')]//a\").get_attribute(\"href\"),\n",
    "        # 6. JS querySelector\n",
    "        lambda el: el.parent.execute_script(\n",
    "            \"return arguments[0].querySelector('.productItemBlock a').href;\", el),\n",
    "        # 7. fallback: scan tất cả <a>\n",
    "        lambda el: next(\n",
    "            (a.get_attribute(\"href\") for a in el.find_elements(By.TAG_NAME, \"a\")\n",
    "             if a.get_attribute(\"href\")), None\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    last_exception = None\n",
    "    for strat_idx, strat in enumerate(strategies, start=1):\n",
    "        for attempt in range(1, max_strategy_retries + 1):\n",
    "            try:\n",
    "                href = strat(prod_element)\n",
    "                if href:\n",
    "                    # Nếu URL là relative (ví dụ bắt được từ data-scroll-target), nối thêm domain\n",
    "                    if href.startswith(\"/\"):\n",
    "                        href = urljoin(BASE_URL, href)\n",
    "                    print(f\"Strategy #{strat_idx} succeeded on attempt {attempt}: {href}\")\n",
    "                    return href\n",
    "                else:\n",
    "                    raise NoSuchElementException(\"Empty href\")\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                print(f\"  Strategy #{strat_idx} attempt {attempt} failed: {e}\")\n",
    "                time.sleep(0.3)\n",
    "        print(f\"→ Strategy #{strat_idx} exhausted, chuyển sang chiến lược tiếp theo.\")\n",
    "    print(f\"Tất cả chiến lược đều thất bại cho phần tử: {last_exception}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_links(page: int, max_retries: int = MAX_RETRIES) -> list:\n",
    "    logger.info(f\"Extracting links from page {page}\")\n",
    "    url = BASE_URL.format(page)\n",
    "    print(f\"Fetching page {page}: {url}\")\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            driver = init_driver()\n",
    "            driver.get(url)\n",
    "\n",
    "            print(\"Scrolling to load products...\")\n",
    "            scroll_down_slowly(driver)\n",
    "\n",
    "            WebDriverWait(driver, IMPLICIT_WAIT).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, \"productItem\"))\n",
    "            )\n",
    "            products = driver.find_elements(By.CLASS_NAME, \"productItem\")\n",
    "            print(f\"  Found {len(products)} product items on page {page}\")\n",
    "\n",
    "            links = []\n",
    "            for idx, prod in enumerate(products, start=1):\n",
    "                href = find_product_link(prod)\n",
    "                if href:\n",
    "                    links.append(href)\n",
    "                else:\n",
    "                    logger.error(f\"Page {page}, product #{idx}: không lấy được link.\")\n",
    "\n",
    "            logger.info(f\"Page {page}: extracted {len(links)} links on attempt {attempt}\")\n",
    "            return links\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt} failed for page {page}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    logger.error(f\"All {max_retries} attempts failed for page {page}\")\n",
    "    return []\n",
    "\n",
    "# -----------------------------------\n",
    "# Detailed product scraping with retry\n",
    "# -----------------------------------\n",
    "def scrape_product_with_retry(url: str, max_retries: int = MAX_RETRIES) -> dict:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            driver = init_driver()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{url}: driver init failed: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            try:\n",
    "                more_btn = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"show-text\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", more_btn)\n",
    "            except Exception:\n",
    "                pass\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"more-detail-content\"))\n",
    "            )\n",
    "            price = driver.find_element(By.CLASS_NAME, \"now-price\").text.strip()\n",
    "            specs = driver.find_elements(By.CLASS_NAME, \"more-detail-content\")\n",
    "            record = {\"URL\": url, \"Price\": price}\n",
    "            for spec in specs:\n",
    "                try:\n",
    "                    label = spec.find_element(By.CLASS_NAME, \"more-label\").text.strip().replace(' ', '_')\n",
    "                    value = spec.find_element(By.CLASS_NAME, \"more-value\").text.strip()\n",
    "                    record[label] = value\n",
    "                except Exception:\n",
    "                    continue\n",
    "            logger.info(f\"Scraped details for {url} on attempt {attempt}\")\n",
    "            return record\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    logger.error(f\"All {max_retries} attempts failed for {url}\")\n",
    "    return {\"URL\": url, \"Error\": \"Failed after retries\"}\n",
    "\n",
    "# -----------------------------------\n",
    "# CSV persistence\n",
    "# -----------------------------------\n",
    "def save_to_csv(record: dict, filename: str = \"output.csv\") -> None:\n",
    "    # Không lưu nếu price là None, '', NaN, hoặc không thể ép về số\n",
    "    price = record.get('Price')\n",
    "    if price in [None, '', 'null', 'None'] or pd.isna(price):\n",
    "        logger.error(f\"Invalid price: {price} for URL: {record.get('URL')}\")\n",
    "        return\n",
    "\n",
    "    # Tạo DataFrame mới và loại bỏ các cột Unnamed\n",
    "    df_new = pd.DataFrame([record])\n",
    "    df_new = df_new.loc[:, ~df_new.columns.str.contains('^Unnamed')]\n",
    "    df_new = df_new.dropna(axis=1, how='all')  # loại cột toàn NaN\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        # Chỉ đọc cột hợp lệ từ file cũ\n",
    "        df_old = pd.read_csv(filename)\n",
    "        df_old = df_old.loc[:, ~df_old.columns.str.contains('^Unnamed')]\n",
    "        df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    else:\n",
    "        df = df_new\n",
    "\n",
    "    # Ghi lại toàn bộ file sau khi append\n",
    "    df.to_csv(filename, index=False)\n",
    "    logger.info(f\"Saved record to {filename}: {record}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Main processing: link extraction & detail scraping with logs and progress bars\n",
    "# -----------------------------------\n",
    "def main():\n",
    "    # # Step 1: Extract links with retry and progress bars\n",
    "    # pages = random.sample(range(1, 500), 912)\n",
    "    # all_links = []\n",
    "    # batches = [pages[i:i + BATCH_SIZE] for i in range(0, len(pages), BATCH_SIZE)]\n",
    "    # for batch_num, batch in enumerate(batches, start=1):\n",
    "    #     logger.info(f\"Starting link batch {batch_num}/{len(batches)}: {batch}\")\n",
    "    #     with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    #         futures = {executor.submit(extract_product_links, p): p for p in batch}\n",
    "    #         for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Links batch {batch_num}\"):\n",
    "    #             page = futures[future]\n",
    "    #             links = future.result()\n",
    "    #             if links and len(links) > 0:\n",
    "    #                 logger.info(f\"Page {page}: found {len(links)} links\")\n",
    "    #                 all_links.extend(links)\n",
    "    #             else:\n",
    "    #                 logger.warning(f\"No links on page {page} after retries\")\n",
    "    #     logger.info(f\"Completed link batch {batch_num}/{len(batches)}\")\n",
    "\n",
    "    # pd.DataFrame({\"Product_URL\": all_links}).to_csv(LINKS_FILE, index=False)\n",
    "    # logger.info(f\"Total links collected: {len(all_links)}\")\n",
    "    # Get all links from CSV\n",
    "    df_links = pd.read_csv(LINKS_FILE)\n",
    "    all_links = df_links[\"Product_URL\"].tolist()[10000:]\n",
    "    # Step 2: Scrape product details in parallel with retry\n",
    "    logger.info(f\"Starting detail scraping for {len(all_links)} products with {MAX_WORKERS} threads\")\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(scrape_product_with_retry, url) for url in all_links]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping products\"):\n",
    "            record = future.result()\n",
    "            save_to_csv(record, \"../data/products2.csv\")\n",
    "    logger.info(\"Scraping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
